---
title: "Assignment2_ClassificationModels"
author: "R.Riddell"
date: "05/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(caret)
library(rattle)
library(pROC)
library(naniar)
library(ggplot2)
library(stringr)
library(lubridate)
```

```{r Read in data, include=FALSE}
df <- read_csv('2014_NBA_shot-data.csv')
```

```{r View Data}
glimpse(df)

# Add some new variables to the dataset
# extracting the date and the matchup information from the matchup variable
Matchup <- data.frame(str_split_fixed(df$matchup," ", n =7)) %>% 
  select(X1,X2,X3,X7) %>% 
  rename(Day = X2,
         Month = X1,
         Year = X3,
         Opposition = X7)
df <- cbind(df,Matchup)
df$date <- paste(df$Year, df$Month, df$Day, sep="-") %>% ymd() %>% as.Date()

# creating a variable to determine if the shot is taken in the final 5 minutes
df <- df %>% 
  mutate(last_5 = if_else(period == 4 & minutes_remaining < 5, 1,0)) 
# a variable to represent if the game goes to overtime
df <- df %>% 
  mutate(Overtime = if_else(period == 5 | period == 6 | period == 7, 1, 0)) 
# binning the close close_def_dist into 5 bins to simplyfy and group into a continous variable
df$close_def_dist_simple <- cut_number(df$close_def_dist,n = 5)

# creating some variables based on the game by summarising player stats into the one observation
ind_game <- df %>% 
  group_by(team_id,date, Opposition) %>% 
  arrange(team_id,date) %>% 
  summarise(across(c(w),~max(.x, na.rm = TRUE))) %>% 
  ungroup()
# creating variables related to win form and total wins
ind_game <- ind_game %>% 
  group_by(team_id) %>% 
  mutate(W_last = if_else(lag(w) == "W",1,0),
         W_last = if_else(is.na(W_last),replace_na(2), W_last),
         win = if_else(w == "W", 1, 0),
         total_wins = if_else(w == "W", cumsum(win) - 1, cumsum(win)),
         streak = if_else(w == 'W', sequence(rle(win)$lengths), as.integer(0)),
         streak = lag(streak),
         streak = if_else(is.na(streak), replace_na(as.integer(0)), streak)) %>% 
  ungroup() %>% 
    mutate_at(vars(W_last,win), as.factor)
# binding the individual game features back to the player dataset
df <- left_join(df,ind_game, by = c("team_id","date","Opposition",'w')) 


# through some later exploration I found these observations that gave 4 or 6 points. I thought the 4 points could be getting fouled on a three point
## attempt but after checking box scores online that did not seem to be the case. 
# I am not sure if these are errors in the dataset or there is some explanation I can't find. 
# As they relate to only two players I am inclined to think it is an error in the data but as i cant be sure I decided to leave it in the data
df %>% 
  select(pts,pts_type) %>% 
  mutate(diff = pts_type - pts) %>% 
  filter(diff < 0)
df %>% 
  filter(pts== 4 | pts ==6)


# removing variables that either will have too much of an impact on modelling (i.e. if pts is left 
## in it directly correlates to made and missed and the models will only use that data), have too many unique observations to assist with modelling 
### or are clearly better represented by other variables
df <- df %>% 
  select(-c(closest_defender,
         player_name,
         case_no,
         game_event_id,
         matchup,
         pts))

# coverting the variables to factors
df <- df %>% 
  mutate_at(vars(location, shot_result,action_type,shot_type,
                 shot_zone_basic, shot_zone_area, shot_zone_range,
                 last_5, Overtime), as.factor)

```

```{r Na exploration}
# this shows that all the missing values relate to the variable shot_clock
sum(is.na(df))
gg_miss_var(df)
vars_with_NA <- miss_var_which(df)

# the majority of these NA values seem to appear when their is less than 24 seconds left in the period.
# due to think I belive they are due to the shot clock being "turned off" so have replaced the NA value with the seconds remaning value.
df %>% 
  select(c(vars_with_NA, seconds_remaining)) %>% 
  gg_miss_fct(fct = seconds_remaining) 
NA_obs <- df %>% 
  filter(is.na(shot_clock)) 
ggplot(NA_obs,aes(minutes_remaining)) + 
  geom_boxplot() 
ggplot(NA_obs,aes(seconds_remaining)) + 
  geom_boxplot()
df$shot_clock <- if_else(is.na(df$shot_clock) & df$seconds_remaining < 24 & df$minutes_remaining == 0, df$seconds_remaining, df$shot_clock)

# the other missing values don't seem to have a patten I can find other than most observations fall in the first three periods
NA_obs <- df %>% 
  filter(is.na(shot_clock)) 
ggplot(NA_obs,aes(period)) + 
  geom_boxplot() 
ggplot(NA_obs,aes(minutes_remaining)) + 
  geom_boxplot() 
ggplot(NA_obs,aes(seconds_remaining)) + 
  geom_boxplot()

# after looking at the relationship between mean shot clock and other variables I settled that period would be the best varible to base the 
## mean values off
df %>% 
  group_by(period) %>%
  summarise(clock = mean(shot_clock, na.rm= T)) %>% 
  ggplot(aes(period,clock)) + geom_point()
# as I am going to do the mean of other varaibles to deal with the remaining NA values I will do it after the data has been split to not affect the 
## out of sample performance

rm(vars_with_NA)
```

```{r split data}
set.seed(100)

# process to split data
# i decided to go with a 70/30 split due to wanting to maximise the amount of numbers to train the model but also give a large amount to compare too
# So i felt a two thirds one third was appropriate 

# this became a bit of an issue with the random forest model as my computer was unable to compute the total training data in any reasonable time
## to combat this is used a 10% subset of the training data to build the random forest model
inTrain <- createDataPartition(y = df$shot_result, p = 0.7, list = F)
training <-  df %>% 
  slice(inTrain)

testing <-  df %>% 
  slice(-inTrain)

dim(training)
dim(testing)

rm(inTrain)
```

```{r NA's, include=FALSE}
# imputing the mean of the period into the NA values and cloneing the data frame
SC_mean_by_period <- training %>% 
  group_by(period) %>%
  mutate(clock = if_else(is.na(shot_clock), mean(shot_clock, na.rm = T), shot_clock)) 

SC_mean_by_period_values <- training %>% 
  group_by(period) %>%
  summarise(clock = mean(shot_clock, na.rm = T))

# calculing the total mean of shot clock and imputing into a cloned data frame for evalution
SC_mean <- if_else(is.na(training$shot_clock), mean(training$shot_clock, na.rm = T), training$shot_clock)

# binding the three columns together to compare the NA values have been filled and what the difference is between column mean and mean by period
NA_check <- cbind(training$shot_clock, SC_mean_by_period$clock, SC_mean)
na_mean_check <- which(is.na(training$shot_clock))
#NA_check[na_mean_check,]

# rounding the imputed values back to two digits
clock <- round(SC_mean_by_period$clock, digits = 2)

# saving over the intial dataframe with the new values
training$shot_clock <- clock

# checking that all the NA values are gone
sum(is.na(training))

# imputing the NA values based of the training set into the testing set
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 1)] <- SC_mean_by_period_values$clock[1]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 2)] <- SC_mean_by_period_values$clock[2]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 3)] <- SC_mean_by_period_values$clock[3]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 4)] <- SC_mean_by_period_values$clock[4]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 5)] <- SC_mean_by_period_values$clock[5]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 6)] <- SC_mean_by_period_values$clock[6]
testing$shot_clock[which(is.na(testing$shot_clock) & testing$period == 7)] <- SC_mean_by_period_values$clock[7]

sum(is.na(testing))

rm(NA_check, SC_mean, SC_mean_by_period, clock, na_mean_check,SC_mean_by_period_values)

training$shot_clock_simple <- cut_interval(training$shot_clock,n = 6)
testing$shot_clock_simple <- cut_interval(testing$shot_clock,n = 6)

```


```{r create variables, include=FALSE}
# I created a range of field goal variables based on other variables in the data
# I am still not 100% if this is appropriate as I realised that a FG% is effectivly a 
## probability of made shot_result

# My concern is whether using the repsonse variable to create a predictor to predict the 
## repsonse is a vaid method

# As the FG% is informed by shot_result but not does not give a 100% prediction accuaracy 
## I decided to use the variables

# FG % based on action type, a concern with this is the observations for action_type are 
## very imbalanced and this FG% will give the same weighting to each action_type even 
### if it appears < .001% of the time
training <- training %>% 
  group_by(action_type) %>% 
  mutate(action_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) %>% 
  ungroup()

training <- training %>% 
  group_by(shot_zone_basic) %>% 
  mutate(shot_zone_basic_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed")))%>% 
  ungroup()

training <- training %>% 
  group_by(shot_zone_range) %>% 
  mutate(shot_zone_range_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) %>% 
  ungroup()

training <- training %>% 
  group_by(pts_type) %>% 
  mutate(FG_2pt_3pt = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) %>% 
  ungroup()

training <- training %>% 
  group_by(period) %>% 
  mutate(period_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) %>% 
  ungroup()

# I did not want to calculte the FG% using values from the training data as I thought that would affect
## the out of sample testing
# So i calculated the amounts based on the training data, saved that as a variable and 
## joined it to the testing data
action_FG <- training %>% 
  group_by(action_type) %>% 
  summarise(action_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) 
testing <- left_join(testing,action_FG, by = 'action_type')

shot_zone_basic_FG <- training %>% 
  group_by(shot_zone_basic) %>% 
  summarise(shot_zone_basic_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed")))
testing <- left_join(testing,shot_zone_basic_FG, by = 'shot_zone_basic')

shot_zone_range_FG <- training %>% 
  group_by(shot_zone_range) %>% 
  summarise(shot_zone_range_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) 
testing <- left_join(testing,shot_zone_range_FG, by = 'shot_zone_range')

FG_2pt_3pt <- training %>% 
  group_by(pts_type) %>% 
  summarise(FG_2pt_3pt = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) 
testing <- left_join(testing,FG_2pt_3pt, by = 'pts_type')

period_FG <- training %>% 
  group_by(period) %>% 
  summarise(period_FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed")))  
testing <- left_join(testing,period_FG, by = 'period')

rm(action_FG,FG_2pt_3pt,period_FG,shot_zone_basic_FG,shot_zone_range_FG)
```

```{r EDA of y, include=FALSE}
# When looking at the response varible it is reasonably balanced with 45% made and 55% missed
training %>%  
  count(shot_result) %>% 
  mutate(prop = prop.table(n)) %>% 
  ggplot(aes(shot_result, prop, fill = shot_result)) +
  geom_bar(stat  = 'identity', position = 'dodge') +
  theme(legend.position = "none") 

```

```{r Boxplot}
gg_boxplot <- function(feature){
  ggplot(training, aes(shot_result,.data[[feature]], fill = shot_result))+
    geom_boxplot()
}

numeric_vars <- training %>% 
select(where(is.numeric), -shot_result) %>% 
  names(.)

for (i in seq_along(numeric_vars)) {
  print(gg_boxplot(numeric_vars[i]))
}
```


```{r Boxplot, include=FALSE}
# These variables did not seem to have an impact on shot_result
training <- training %>% 
  select(-c(game_id,
            shot_number,
            minutes_remaining,
            seconds_remaining))

```


```{r Boxplot}
gg_barplot <- function(feature){
  ggplot(training, aes(.data[[feature]], fill = shot_result))+
    geom_bar(position = 'dodge')+
    xlab(paste(feature))+
    theme(axis.text.x = element_text(angle = 45))
}

factor_vars <- training %>% 
select(where(is.factor), -shot_result) %>% 
  names(.)

for (i in factor_vars) {
  print(gg_barplot(i))
}
```

```{r}
# When looking at the action_type we can see a very imbalanced set of observations
training %>% 
  group_by(action_type) %>% 
  count() %>% 
  mutate(Percet_obs = n/ (length(training$action_type))) %>% 
  arrange(-n)
# as action_type informs action_type_FG I decided to remove action_type to improve processing
## time as less dummy values are created and therefore less total variables
training <-  training %>% 
  select(-action_type)

```


```{r Boxplot, include=FALSE}
# These variables do not seem to have a clear affect on make or miss
training <- training %>% 
  select(-Year)
```



```{r Scatterplot, eval=FALSE, include=FALSE}
# created a function to visualise the FG attributes and if they change based on the variable ## they are related too
gg_scatter <- function(feature){
  training %>% 
  group_by(.data[[feature]]) %>% 
  mutate(FG = (sum(shot_result == 'made')) / (sum(shot_result == 'made') + sum(shot_result == "missed"))) %>% 
  ggplot(aes(.data[[feature]], FG)) +
    geom_point() +
    xlab('shot result') +
    theme(axis.text.x = element_text(angle = 45, vjust = 1))  
  }

created_vars <- c('shot_zone_range', 'action_type', 'shot_zone_basic', 'pts_type', 'period', 'streak','total_wins')

for (i in seq_along(created_vars)) {
  print(gg_scatter(created_vars[i]))
}

```



```{r NZV}
# overtime returns a near zero value as so few games go into overtime
nearZeroVar(training, names = T)

training %>% 
  group_by(Overtime) %>% 
  count()

training <- training %>% 
  select(-Overtime)
```

```{r multicolinearity}
## create a correlation matrix of all the numeric variables
cor_mat <- training %>% 
  select(where(is.numeric)) %>% 
  cor(., method = 'spearman')

# visualise the correlation matrix
GGally::ggcorr(cor_mat)
# shot_distance, shot_dist and loc_y are highly correlated as they all refer to the distance from basket
# pts_type and distance features are correlated as to get 3 points you must be a certain distance away
# period and shot_number are correlated becasue as the period increases therefore number of shots will also
# dribbles and touch time are highly correlated as you need to increase touch time to dribble
# close_def_dist and distance is also highly correlated as when you are further from the ring
## there is more space for the closest defender to be away.
# close_def_dist, touch time and distance are correlated as when the defender is closer you are less likely to dribble

## select the variables that display multicolinearty above 0.8
cor_features <- findCorrelation(cor_mat, cutoff = 0.8, names = T, exact = FALSE)
# shot distance and dribbles are selected have high multicolinearity and have less of an impact on the overall data 

## remove the variables that display multicolinearty
training <- training %>% 
  select(-all_of(cor_features))
```

```{r}
glimpse(training)

# these variables have been selected to be removed as they appear to be better represented by otehr varibles
training <- training %>% 
  select(-c(close_def_dist_simple,shot_zone_basic,FG_2pt_3pt))

training <- training %>% 
  select(-c(Opposition, Month, Day))
```


--------------------------------------------------------------------------------------------------------------------------------------

```{r Cross validation, include=FALSE}
# establshing a control_obj for modelling
# the repeatedcv was used to attain the figures listed but took a significant amount of time and did not significantly improve the result
# So most interations were done using standard cross validation with 10 folds
control_obj <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 5,
  savePredictions = "final",
  classProbs = T, 
  summaryFunction = twoClassSummary
)

control_obj <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = T, 
  summaryFunction = twoClassSummary
)

```


```{r Logistic Regression}
# running a logistic regression model
set.seed(345)
mdl_logreg <- train(shot_result~.,
                    data = training,
                    method = "glm",
                    family = 'binomial',
                    trControl = control_obj)
mdl_logreg
# The logistic regression model returned the values ROC:0.69 Sens:0.45 Spec:0.82
# as the sensitivty is 0.44 we can assume the model will be poor at predicting made shots
# the higher spec value shows the model is better an predicting a missed shot
```


```{r confusion matrix}
confusionMatrix(data = mdl_logreg,
                reference = training$shot_result,
                positive = "made")

# accuracy on the testing set is 65%, clearly the model is better at predicting missed shows as opposed to made shots
# with the model incorrectly predicting more made as missed than correctly predicting made as made
```


```{r Decsion tree}
# running a decsion tree model
set.seed(345)

tree_mdl  <- train(shot_result ~. , data = training ,
                   method = 'rpart',
                   tuneGrid = expand.grid(cp = seq(0.001,0.004,0.0001)),
                   trControl = control_obj,
                   metric = "ROC")

tree_mdl
plot(tree_mdl)
rattle::fancyRpartPlot(tree_mdl$finalModel, sub = "", palettes = 'RdBu')
plot(varImp(tree_mdl), top =20)
# The decsion tree model found the best cp = 0.0024, as the cp incresaes from this point we see a decrease in ROC
# Values returned were ROC: 0.66  Sens: 0.46  Spec: 0.82
# as the sensitivty is 0.46 we can assume the model will be poor at predicting made shots
# the higher spec value shows the model is better an predicting a missed shot

# as we can see in the variable importance plot the action_FG is very important when splits are made
# behind the action_FG we see the loc_y and shot_zone_basic_FG as very important as we have seen this is a measure of distance from the ring
# after the distance variables teh touch time, shot clock and close def dist have some importance but signifincatly less
# this steep drop off in variable importance makes sense when we look at how the cp drops when more varibles increase
```


```{r Confusion matrix}
confusionMatrix(data = tree_mdl,
                reference = training$shot_result,
                positive = "made")

# accuracy on the testing set is 66%, clearly the model is better at predicting missed shows as opposed to made shots
# with the model incorrectly predicting more made as missed than correctly predicting made as made
```


```{r random forest}
# running a random forest model
# due to the significant amount or processing timem I did most of my iterations were performed
## on a random sample of 14000 observations from the training data
set.seed(345)

inTrain <- createDataPartition(y = training$shot_result, p = 0.1, list = F)

rf_training <-  training %>% 
  slice(inTrain)

rf_mdl  <- train(shot_result ~. , data = rf_training ,
                   method = 'ranger',
                   trControl = control_obj,
                   verbose = FALSE)
                   

rf_mdl
plot(rf_mdl)
# The random forest model found the best mtry = 19 and using the splitrule = gini
# Values returned were ROC:0.70 Sens:0.50 Spec:0.79
# as the sensitivty is 0.49 we can assume the model will be poor at predicting made shots
# the higher spec value shows the model is better an predicting a missed shot
```


```{r Confusion matrix}
confusionMatrix(data = rf_mdl,
                reference = training$shot_result,
                positive = "made")
# accuracy on the testing set is 66%, clearly the model is better at predicting missed shows as opposed to made shots
# this model shows an improvement on predicting made shots
```


```{r in sample performance}
# comparing the model performances
resamps <- resamples(list(tree = tree_mdl,
                          rf = rf_mdl,
                          logreg = mdl_logreg))

bwplot(resamps)
summary(resamps)

# I deceided to use the random forest model on the testing data.
# as the random forest had a similar ROC to the Logistic model and better than the decsion tree
# The random forest also had a better mean sens (~ +0.03 -0.05) but a slightly slower spec(~ -0.02)
# I decided this was a fair trade off as it was more important as the gain from sens was more than the loss to spec
# As the models are all worse at predicting made shots i thought it was best to take the one that was the best at it
```

```{r Apply to test}
# actual predictions
predictions_rf <- predict(rf_mdl, newdata = testing, type = "raw")

# probabilties of seeing each result
prob_rf <- predict(rf_mdl, newdata = testing, type = "prob")

```


```{r Confusion Matrix}
# Confusion matrix
confusionMatrix(data = predictions_rf,
                reference = testing$shot_result,
                positive = "made")
# As we saw with the spec and sens the model correctly predicted twice as many misses than
## makes, even though there is only 10% more misses in the dataset

# the model predicted half of the made shots to be misses and 20% of the misses to be makes which
## is the raw outcome of our spec and sens.
# the overall accuracy of the model was 65% which is better than a coin flip but if you are
## wanting to predict only made shots you may as well flip a coin
```


```{r ROC Curve}
made_shot_binary <- if_else(testing$shot_result == "made",1,0)

prob_rf <- prob_rf %>% 
  select('made') %>% 
  unlist() %>% 
  unname()

pROC::roc(response = made_shot_binary,
          predictor = prob_rf,
          ci = T,
          plot = T,
          legacy.axes = T,
          print.auc = T,
          print.thres = 0.5,
          asp = NA)
# due to our lower sens values the AUC is affected, to improve the AUC we would have to
## devise a way to more accucartly predict made shots (true positives)

# Overall I belive the model had difficulty predicting made shots becasue in a real world
## situation people players will often make lower probability shots (like a long 3pt shot) so
### in that situation the model will predict a miss when it is a make (ie false negative)

# But players are less likely to miss a high probability shot (closer than 8ft) so due to the
## high probability the model is less to predict a miss when it is a make (ie false positive)

# This means that the model will predict significatly more misses and therefore get a higher
## accucarcy on misses but the trade off is a low accucarcy on makes. 

```

